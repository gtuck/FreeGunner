name: Daily Article Researcher

on:
  schedule:
    - cron: "0 9 * * *"   # 9:00 AM UTC
  workflow_dispatch:

permissions:
  contents: write

jobs:
  run:
    runs-on: ubuntu-latest
    env:
      DOTENV_PATH: .env
      FEEDS_FILE: feeds.json
      HISTORY_FILE: history.json
      OUTPUT_DIR: articles
      RESULTS_PER_QUERY: "8"
      USER_AGENT: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125 Safari/537.36"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml python-dotenv

      - name: Fetch, research, and summarize
        id: fetch
        shell: bash
        run: |
          set -euo pipefail

          cat > action_main.py << 'PY'
          import os, sys, json, csv, re, time
          from datetime import datetime, timezone
          from pathlib import Path
          import requests
          from xml.etree import ElementTree as ET
          from bs4 import BeautifulSoup

          def getenv(k, d=None): 
              v = os.getenv(k, d); 
              return v if v is not None else d

          # Load .env (best effort)
          try:
              from dotenv import load_dotenv
              dp = getenv("DOTENV_PATH", ".env")
              if Path(dp).is_file(): load_dotenv(dp)
          except Exception:
              pass

          FEEDS_FILE   = getenv("FEEDS_FILE", "feeds.json")
          HISTORY_FILE = getenv("HISTORY_FILE", "history.json")
          OUTPUT_DIR   = getenv("OUTPUT_DIR", "articles")
          RESULTS_PER_QUERY = int(getenv("RESULTS_PER_QUERY", "8"))
          USER_AGENT   = getenv("USER_AGENT", "Mozilla/5.0")
          GCS_API_KEY  = getenv("GCS_API_KEY", "")
          GCS_CX       = getenv("GCS_CX", "")
          DEFAULT_RSS  = getenv("DEFAULT_RSS", "https://www.firearmsnews.com/RSS.aspx?websiteid=77508&listingid=77589")

          sess = requests.Session()
          sess.headers.update({"User-Agent": USER_AGENT, "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"})

          def read_feeds(p: Path):
              if not p.exists():
                  data = [{"FeedURL": DEFAULT_RSS, "Active": True}]
                  p.write_text(json.dumps(data, indent=2))
                  return data
              if p.suffix.lower() == ".json":
                  return json.loads(p.read_text())
              out=[]
              with p.open(newline="", encoding="utf-8") as f:
                  r=csv.DictReader(f)
                  for row in r:
                      url=(row.get("FeedURL") or "").strip()
                      act=(row.get("Active") or "true").strip().lower() in ("1","true","yes","y")
                      if url: out.append({"FeedURL": url, "Active": act})
              return out

          def read_history(p: Path):
              if not p.exists(): return set()
              if p.suffix.lower() == ".json":
                  try: return set(json.loads(p.read_text()))
                  except Exception: return set()
              links=set()
              with p.open(newline="", encoding="utf-8") as f:
                  for row in csv.DictReader(f):
                      link=(row.get("Link") or "").strip()
                      if link: links.add(link)
              return links

          def write_history(p: Path, links: set):
              if p.suffix.lower() == ".json":
                  p.write_text(json.dumps(sorted(links), indent=2))
              else:
                  with p.open("w", newline="", encoding="utf-8") as f:
                      w=csv.writer(f); w.writerow(["Link"])
                      for L in sorted(links): w.writerow([L])

          def fetch_rss_first_item(url: str):
              for a in range(3):
                  try:
                      r=sess.get(url, timeout=30); r.raise_for_status()
                      root=ET.fromstring(r.content)
                      it=root.find(".//item")
                      if it is None: return None
                      def gx(t):
                          el=it.find(t); return el.text.strip() if el is not None and el.text else ""
                      return {"title": gx("title"), "link": gx("link"), "pubDate": gx("pubDate"), "description": gx("description")}
                  except Exception as e:
                      if a==2: 
                          print(f"[RSS ERROR] {url}: {e}", file=sys.stderr); return None
                      time.sleep(5)

          def gsearch(q: str, n: int):
              if n
